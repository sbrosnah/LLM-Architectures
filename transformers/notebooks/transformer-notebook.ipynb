{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "N_ENCODER_LAYERS = 6\n",
    "N_DECODER_LAYERS = 6\n",
    "D_MODEL = 512\n",
    "D_QUERY_KEY = 256\n",
    "D_VALUE = 256\n",
    "D_EMBED = 256\n",
    "\n",
    "# Other settings \n",
    "BATCH_DIMENSION = 0 # number of sentences\n",
    "SEQUENCE_DIMENSION = 1 # number of words per sentence\n",
    "FEATURE_DIMENSION = 2 # word vector dimensionality\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else: \n",
    "        return torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index into the batch dimension to get a given sentence. \n",
    "Theo batch dimension is named how it is because including it as a whole gives us the whole batch which is made up of sentences\n",
    "\n",
    "Index into the sequence dimension to get a given word vector \n",
    "Index into the feature dimension to get a given number in the word vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_one_sentences = [\n",
    "    \"hello there, you are funny\",\n",
    "    \"why is my watter bottle brown?\",\n",
    "    \"who pooped on the floor!?\",\n",
    "    \"peaches, peaches, peaches, peaches\"\n",
    "]\n",
    "\n",
    "batch_two_senteces = [\n",
    "    \"why so serious?\",\n",
    "    \"darkness is my ally\",\n",
    "    \"who pooped on the floor!?\"\n",
    "]\n",
    "\n",
    "batches = [batch_one_sentences, batch_two_senteces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 16, 16, 17]], [[5, 18, 19], [20, 6, 7, 21], [11, 12, 13, 14, 15]]]\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "for b in batches:\n",
    "    for s in b:\n",
    "        for w in s.split():\n",
    "            if w not in word_to_index:\n",
    "                word_to_index[w] = len(word_to_index)\n",
    "\n",
    "tokenized_sentences = [[[word_to_index[word] for word in sentence.split()] for sentence in batch] for batch in batches]\n",
    "\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep (padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [0], [0], [0], [0], [0]]\n"
     ]
    }
   ],
   "source": [
    "print([[1]] + [[0]] * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad the batches and sentences to be the same length\n",
    "max_len = max(len(b) for b in batches)\n",
    "padded_tokenized_sentences = [batch + [[0]] * (max_len - len(batch)) for batch in tokenized_sentences]\n",
    "\n",
    "for i in range(len(tokenized_sentences)):\n",
    "    max_len = max(len(sentence) for sentence in tokenized_sentences[i])\n",
    "    padded_tokenized_sentences[i] = [sentence + [0] * (max_len - len(sentence)) for sentence in tokenized_sentences[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 1, 2, 3, 4, 0],\n",
       "  [5, 6, 7, 8, 9, 10],\n",
       "  [11, 12, 13, 14, 15, 0],\n",
       "  [16, 16, 16, 17, 0, 0]],\n",
       " [[5, 18, 19, 0, 0], [20, 6, 7, 21, 0], [11, 12, 13, 14, 15]]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(padded_tokenized_sentences[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 6])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor(padded_tokenized_sentences[0]) != 0).unsqueeze(-2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  True,  True, False]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[ True,  True,  True,  True,  True, False]],\n",
       "\n",
       "        [[ True,  True,  True,  True, False, False]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor(padded_tokenized_sentences[0]) != 0).unsqueeze(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0,  1,  2,  3,  4,  0],\n",
       "         [ 5,  6,  7,  8,  9, 10],\n",
       "         [11, 12, 13, 14, 15,  0],\n",
       "         [16, 16, 16, 17,  0,  0]]),\n",
       " tensor([[ 5, 18, 19,  0,  0],\n",
       "         [20,  6,  7, 21,  0],\n",
       "         [11, 12, 13, 14, 15]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_training_batches = [torch.tensor(batch, dtype=torch.long) for batch in padded_tokenized_sentences]\n",
    "torch_training_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 10])\n"
     ]
    }
   ],
   "source": [
    "#Map inputs to embeddings\n",
    "\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "embedding_dim = 10\n",
    "\n",
    "#matrix of size vocab_size x embedding_dim is initualized with random weights\n",
    "#In training, the embeddings are adjusted with the rest of the parameters in the model\n",
    "embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "for i in range(len(torch_training_batches)):\n",
    "    torch_training_batches[i] = embed(torch_training_batches[i])\n",
    "\n",
    "print(torch_training_batches[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(22, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### practice working with fake training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.3601, -0.5811, -0.7539, -0.1906,  0.7925,  0.3993,  1.7720, -0.3877,\n",
       "         0.0407,  1.6659], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# give me the embedding for second word in the second sentence of the second batch\n",
    "torch_training_batches[1][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the second batch and first sentence \n",
    "torch_training_batches[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the positonal Encoding Layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figuring out matrix multiplication dimensions with dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -4.,  -4.,  -8.,  -8., -12., -12., -12.],\n",
       "          [ -8.,  -8., -16., -16., -24., -24., -24.],\n",
       "          [-12., -12., -24., -24., -36., -36., -36.]],\n",
       "\n",
       "         [[  0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "          [  4.,   4.,   8.,   8.,  12.,  12.,  12.],\n",
       "          [  8.,   8.,  16.,  16.,  24.,  24.,  24.]]],\n",
       "\n",
       "\n",
       "        [[[  4.,   4.,   8.,   8.,  12.,  12.,  12.],\n",
       "          [  8.,   8.,  16.,  16.,  24.,  24.,  24.],\n",
       "          [ 12.,  12.,  24.,  24.,  36.,  36.,  36.]],\n",
       "\n",
       "         [[  0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "          [ -4.,  -4.,  -8.,  -8., -12., -12., -12.],\n",
       "          [ -8.,  -8., -16., -16., -24., -24., -24.]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_heads = 2\n",
    "n_sentences = 2\n",
    "n_words = 3\n",
    "d_embed = 4\n",
    "d_q_k = 2\n",
    "d_v = 3\n",
    "\n",
    "\n",
    "batch = torch.tensor([[[1, 1, 1, 1],\n",
    "                        [2, 2, 2, 2],\n",
    "                        [3, 3, 3, 3]],\n",
    "                        [[0, 0, 0, 0],\n",
    "                         [-1, -1, -1, -1],\n",
    "                         [-2, -2, -2, -2]]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "head_1_q_k_v_weights = torch.tensor([\n",
    "    [-1, -1, -2, -2, -3, -3, -3],\n",
    "    [-1, -1, -2, -2, -3, -3, -3],\n",
    "    [-1, -1, -2, -2, -3, -3, -3],\n",
    "    [-1, -1, -2, -2, -3, -3, -3]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "head_2_q_k_v_weights = torch.tensor([\n",
    "    [1, 1, 2, 2, 3, 3, 3],\n",
    "    [1, 1, 2, 2, 3, 3, 3],\n",
    "    [1, 1, 2, 2, 3, 3, 3],\n",
    "    [1, 1, 2, 2, 3, 3, 3]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "head_1_q_k_v = torch.matmul(batch, head_1_q_k_v_weights)\n",
    "head_2_q_k_v = torch.matmul(batch, head_2_q_k_v_weights)\n",
    "manual_result = torch.stack([head_1_q_k_v, head_2_q_k_v])\n",
    "manual_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -4.,  -4.,  -8.,  -8., -12., -12., -12.],\n",
       "          [ -8.,  -8., -16., -16., -24., -24., -24.],\n",
       "          [-12., -12., -24., -24., -36., -36., -36.]],\n",
       "\n",
       "         [[  0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "          [  4.,   4.,   8.,   8.,  12.,  12.,  12.],\n",
       "          [  8.,   8.,  16.,  16.,  24.,  24.,  24.]]],\n",
       "\n",
       "\n",
       "        [[[  4.,   4.,   8.,   8.,  12.,  12.,  12.],\n",
       "          [  8.,   8.,  16.,  16.,  24.,  24.,  24.],\n",
       "          [ 12.,  12.,  24.,  24.,  36.,  36.,  36.]],\n",
       "\n",
       "         [[  0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "          [ -4.,  -4.,  -8.,  -8., -12., -12., -12.],\n",
       "          [ -8.,  -8., -16., -16., -24., -24., -24.]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_weights = torch.stack([head_1_q_k_v_weights, head_2_q_k_v_weights])\n",
    "matrix_result = torch.matmul(batch, torch.unsqueeze(head_weights, 1))\n",
    "matrix_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figuring out how to add a bias to heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(torch.stack([torch.ones((1, 3)), torch.zeros((1, 3))]), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 1, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.ones((1, 3)), torch.zeros((1, 3))]).unsqueeze(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2., 2., 2.],\n",
       "          [2., 2., 2.]],\n",
       "\n",
       "         [[2., 2., 2.],\n",
       "          [2., 2., 2.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       "\n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2, 2, 2, 3)) + torch.unsqueeze(torch.stack([torch.ones((1, 3)), torch.zeros((1, 3))]), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape:  torch.Size([2, 3, 4])\n",
      "weight shape:  torch.Size([2, 1, 4, 7])\n",
      "result shape:  torch.Size([2, 2, 3, 7])\n"
     ]
    }
   ],
   "source": [
    "print(\"batch shape: \", batch.shape)\n",
    "print(\"weight shape: \", torch.unsqueeze(head_weights, 1).shape)\n",
    "print(\"result shape: \", torch.matmul(batch, torch.unsqueeze(head_weights, 1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 1., 4., 7.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor((n_heads, 1, 4, d_q_k * 2 + d_v), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 7])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(torch.stack([torch.ones((1, d_q_k * 2 + d_v)), torch.zeros((1, d_q_k * 2 + d_v))]), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figuring out how to split the query, key, weight result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, k, v = torch.split(matrix_result, [d_q_k, d_q_k, d_v], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q shape:  torch.Size([2, 2, 3, 2])\n",
      "k shape:  torch.Size([2, 2, 3, 2])\n",
      "v shape:  torch.Size([2, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"q shape: \", q.shape)\n",
    "print(\"k shape: \", k.shape)\n",
    "print(\"v shape: \", v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to use transpose to swap dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2, 3])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(k, 2, 3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figuring out the softmax dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = nn.Softmax(dim=-1)\n",
    "t = torch.stack([torch.tensor([1, 1], dtype=torch.float), torch.tensor([3, 5], dtype=torch.float)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000],\n",
       "        [0.1192, 0.8808]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments to add back in for testing purposes if needed\n",
    "\n",
    "# self.weight = nn.Parameter(head_weights.unsqueeze(dim=1))\n",
    "# self.bias = nn.Parameter(torch.unsqueeze(torch.stack([torch.ones((1, d_q_k * 2 + d_v)), torch.zeros((1, d_q_k * 2 + d_v))]), dim=1)) if bias else None\n",
    "#         self.bias = nn.Parameter(torch.rand((n_heads, 1, 1, d_q_k * 2 + d_v), dtype=torch.float32)) if bias else None\n",
    "\n",
    "        # if self.bias is not None:\n",
    "        #     fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        #     bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "        #     nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "        # if self.bias != None:\n",
    "        #     output += self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLinearLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_out, bias=False):\n",
    "        super(MultiHeadLinearLayer, self).__init__()\n",
    "        for key, value in list(locals().items())[1:4]:\n",
    "            setattr(self, key, value)\n",
    "\n",
    "        self.weight = nn.Parameter(torch.rand((n_heads, 1, d_model, d_out), dtype=torch.float32))\n",
    "        self.bias = nn.Parameter(torch.rand((n_heads, 1, 1, d_out), dtype=torch.float32)) if bias else None\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = torch.matmul(input, self.weight)\n",
    "        if self.bias != None:\n",
    "            output += self.bias\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Multi head attention layer\n",
    "\n",
    "'''\n",
    "[description]\n",
    "\n",
    "Parameters:\n",
    "\n",
    "Returns:\n",
    "\n",
    "Raises:\n",
    "\n",
    "Example:\n",
    "\n",
    "'''\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, d_model: int, d_q_k: int, d_v: int):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        for key, value in list(locals().items())[1:5]:\n",
    "            setattr(self, key, value)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.query_weights = MultiHeadLinearLayer(n_heads, d_model, d_q_k)\n",
    "        self.key_weights = MultiHeadLinearLayer(n_heads, d_model, d_q_k)\n",
    "        self.value_weights = MultiHeadLinearLayer(n_heads, d_model, d_v)\n",
    "\n",
    "        self.linear_out = nn.Linear(in_features=n_heads * d_v, out_features=d_model, bias=True)\n",
    "\n",
    "    def _scaled_dot_product_attention(self, q, k, v):\n",
    "        return torch.matmul(self.softmax(torch.matmul(q, torch.transpose(k, -2, -1)) / torch.sqrt(torch.tensor(q.shape[-1], dtype=torch.float32))), v)\n",
    "\n",
    "    def forward(self, x, x_encoder=None):\n",
    "\n",
    "        q = self.query_weights(x)\n",
    "\n",
    "        if x_encoder:\n",
    "            k = self.key_weights(x_encoder)\n",
    "            v = self.value_weights(x_encoder)\n",
    "        else:\n",
    "            k = self.key_weights(x)\n",
    "            v = self.value_weights(x)\n",
    "        \n",
    "        values = self._scaled_dot_product_attention(q, k, v)\n",
    "        concatenated = torch.cat([v for v in values], dim=-1)\n",
    "\n",
    "        return self.linear_out(concatenated)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AddAndNorm, self).__init__()\n",
    "        self.norm = nn.functional.layer_norm\n",
    "\n",
    "    def forward(self, prev, curr):\n",
    "        return self.norm(prev + curr, prev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self,in_features: int, hidden_features: int, out_features: int):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        self.input_layer = nn.Linear(in_features=in_features, out_features=hidden_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(in_features=hidden_features, out_features=out_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.output_layer(self.relu(self.input_layer(x)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads: int, d_model: int, d_q_k: int, d_v: int, d_ff: int):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        for key, value in list(locals().items())[1:6]:\n",
    "            setattr(self, key, value)\n",
    "            \n",
    "        self.multi_head_attention = MultiHeadAttention(n_heads, d_model, d_q_k, d_v)\n",
    "        self.add_and_norm = AddAndNorm()\n",
    "        self.feed_forward = FeedForwardLayer(in_features=d_model, hidden_features=d_ff, out_features=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_1 = self.add_and_norm(x, self.multi_head_attention(x))\n",
    "        return self.add_and_norm(out_1, self.feed_forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layers: int, n_heads: int, d_model: int, d_q_k: int, d_v: int, d_ff: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        for key, value in list(locals().items())[1:7]:\n",
    "            setattr(self, key, value)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(n_heads, d_model, d_q_k, d_v, d_ff) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_device()\n",
    "m_h_attention = MultiHeadAttention(2, 4, 2, 3)\n",
    "m_h_attention.to(device)\n",
    "o = m_h_attention(batch.to(device))\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 50, 512])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.rand((100, 50, 512))\n",
    "encoder_layer = EncoderLayer(8, 512, 64, 64, 2048)\n",
    "encoder_layer.to(device)\n",
    "encoder_layer(test.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 50, 512])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Encoder Layer\n",
    "encoder = Encoder(6, 8, 512, 64, 64, 2048)\n",
    "encoder.to(device)\n",
    "encoder(test.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18892800"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "total_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Masked Multi-head attention layer\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, d_model: int, d_q_k: int, d_v: int):\n",
    "        super(MaskedMultiHeadAttention, self).__init__()\n",
    "        for key, value in list(locals().items())[1:5]:\n",
    "            setattr(self, key, value)\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(n_heads, d_model, d_q_k, d_v)\n",
    "    \n",
    "    def add_mask(self, x):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.multi_head_attention(self.add_mask(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Decoder Layer (Take the keys and values from the last encoder layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Softmax Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it all together \n",
    "\n",
    "# based on the dimension of the input, change the batch, sequence, and feature dimensions as needed. Set to none if non-existent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
