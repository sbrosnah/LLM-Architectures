{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_q_k, d_v):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        for key, value in list(locals().items())[1:5]:\n",
    "            setattr(self, key, value)\n",
    "\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_q_k * num_heads)\n",
    "        self.W_k = nn.Linear(d_model, d_q_k * num_heads)\n",
    "        self.W_v = nn.Linear(d_model, d_v * num_heads)\n",
    "        self.W_o = nn.Linear(d_v * num_heads, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_q_k)\n",
    "        if mask is not None:\n",
    "            #Set the masked values such that when they are passed through the softmax, they\n",
    "            #become 0\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x, d):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        #view splits the embedding dimension \n",
    "        return x.view(batch_size, seq_length, self.num_heads, d).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_v * self.num_heads)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        Q = self.split_heads(self.W_q(q), self.d_q_k)\n",
    "        K = self.split_heads(self.W_k(k), self.d_q_k)\n",
    "        V = self.split_heads(self.W_v(v), self.d_v)\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(6, 8, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[1, 2, 3, 4, 5, 6],\n",
    "                    [7, 8, 9, 10, 11, 12]],\n",
    "                    [[13, 14, 15, 16, 17, 18],\n",
    "                     [19, 20, 21, 22, 23, 24]]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.9677, -1.4866, -2.8680, -4.3427, -0.9212,  1.6678],\n",
       "         [-3.0570, -1.8421, -2.7074, -4.2729, -1.0963,  1.8373]],\n",
       "\n",
       "        [[-6.9505, -0.9727, -7.2178, -9.3986, -2.5982,  4.5516],\n",
       "         [-6.8928, -0.9621, -7.3239, -9.3857, -2.6173,  4.5706]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha(x, x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain the split heads method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1,  2],\n",
       "          [ 3,  4],\n",
       "          [ 5,  6]],\n",
       "\n",
       "         [[ 7,  8],\n",
       "          [ 9, 10],\n",
       "          [11, 12]]],\n",
       "\n",
       "\n",
       "        [[[13, 14],\n",
       "          [15, 16],\n",
       "          [17, 18]],\n",
       "\n",
       "         [[19, 20],\n",
       "          [21, 22],\n",
       "          [23, 24]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_length, d_model = x.size()\n",
    "\n",
    "#split up the embedding dimension into the heads. Here, our embedding dimension is 6 and we have 3 heads. \n",
    "x.view(batch_size, seq_length, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1,  2],\n",
       "          [ 3,  4],\n",
       "          [ 5,  6]],\n",
       "\n",
       "         [[ 7,  8],\n",
       "          [ 9, 10],\n",
       "          [11, 12]]],\n",
       "\n",
       "\n",
       "        [[[13, 14],\n",
       "          [15, 16],\n",
       "          [17, 18]],\n",
       "\n",
       "         [[19, 20],\n",
       "          [21, 22],\n",
       "          [23, 24]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We then shuffle the data by switching the sequence dimension with the head dimension so that we can have all the token embeddings \n",
    "# in one head instead of all head embeddings for each token\n",
    "split_heads = x.view(batch_size, seq_length, 3, 2).transpose(1, 2)\n",
    "split_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain the combine heads function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1,  2],\n",
       "          [ 3,  4],\n",
       "          [ 5,  6]],\n",
       "\n",
       "         [[ 7,  8],\n",
       "          [ 9, 10],\n",
       "          [11, 12]]],\n",
       "\n",
       "\n",
       "        [[[13, 14],\n",
       "          [15, 16],\n",
       "          [17, 18]],\n",
       "\n",
       "         [[19, 20],\n",
       "          [21, 22],\n",
       "          [23, 24]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch, head, seq, emb -> batch, seq, head, emb - This just switches the dimensions back\n",
    "split_heads.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4,  5,  6],\n",
       "         [ 7,  8,  9, 10, 11, 12]],\n",
       "\n",
       "        [[13, 14, 15, 16, 17, 18],\n",
       "         [19, 20, 21, 22, 23, 24]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make the new tensor contiguous for fast retrieval of the new tensor\n",
    "#reshuffle the data so that it is in the same shape as befor the split. This merges the heads\n",
    "split_heads.transpose(1, 2).contiguous().view(batch_size, seq_length, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-Wise Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # create a placeholder for the positional encodings\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        # create a column array that acts as a scaling factor for the sin and cosine computations. The idea is that the position will effect the frequency of the function \n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # create a row array to multiply with the position matrix. shape should be (1, d_model / 2) because we will use the same values for both the sine and cosine computations\n",
    "        # which are interleaved\n",
    "        # these are the positons within the encoding vector where the sine and cosine functions will be applied \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        \n",
    "        #interleave the sin and cosine computation in our placeholder\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) #put the sine computations in the evenly indexed columns for every row\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) #put the cosine computations in the odd indexed columns for every row\n",
    "        \n",
    "        #registers a tensor as a buffer, not a parameter. This is not updated by the optimizer\n",
    "        # Automatically will be included in the model's state_dict\n",
    "        # using register buffer allows for automatic device management. \n",
    "        # This will also be included in the computation graph so that all parameters recieve the correct credit for the outputs\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  7.3891, 54.5981])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.exp(torch.arange(0, d_model, 2).float())\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5350567286626973"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = -(math.log(10000.0) / d_model)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -1.5351, -11.3426, -83.8113])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 7.3891e+00, 5.4598e+01, 4.0343e+02, 2.9810e+03])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.arange(0, 10, 2).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9210340371976183"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(math.log(10000.0) / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.2103e-01, -6.8056e+00, -5.0287e+01, -3.7157e+02, -2.7456e+03])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.arange(0, 10, 2).float()) * -(math.log(10000.0) / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [1.0000e+00, 4.6416e-02, 2.1544e-03],\n",
       "        [2.0000e+00, 9.2832e-02, 4.3089e-03],\n",
       "        [3.0000e+00, 1.3925e-01, 6.4633e-03],\n",
       "        [4.0000e+00, 1.8566e-01, 8.6177e-03]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = 5\n",
    "torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1) * torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.0464, 0.0022])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout: regularization technique where randomly selected neurons are ignored during training. This helps to reduce overfitting by making the neural network less sensitive to the specific weights of neurons. This forces the network to learn more distributed representations.\n",
    "\n",
    "Residual connections: Help to avoid the vanishing gradient problem. \n",
    "\n",
    "Layer Normalization: normalizes the inputs across the features instead of the batch dimension. Ensures that the mean and variance are stable across the features which helps to stabilize the learning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_q_k, d_v, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, d_q_k, d_v)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, d_q_k, d_v)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        #maske the padding tokens\n",
    "        src_mask = (src != 0).unsqueeze(-1)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(-1)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        #the final target mask consists of the padding mask and the no peak mask\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 4\n",
    "batch_size = 2\n",
    "seq_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 6],\n",
       "        [4, 4, 2]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_batch = torch.randint(1, 10, (batch_size, seq_length))\n",
    "tokenized_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 0],\n",
       "        [4, 4, 0]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_batch[:, -1] = 0 #symbolic of padding\n",
    "tokenized_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True],\n",
       "         [ True],\n",
       "         [False]],\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [False]]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask = (tokenized_batch != 0).unsqueeze(-1)\n",
    "src_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7, 4, 6, 3],\n",
       "         [4, 2, 6, 3],\n",
       "         [4, 2, 2, 3]],\n",
       "\n",
       "        [[4, 3, 1, 3],\n",
       "         [6, 4, 9, 2],\n",
       "         [3, 7, 3, 2]]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_batch = torch.randint(1, 10, (batch_size, seq_length, embedding_size))\n",
    "embedded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7,  4,  6,  3],\n",
       "         [ 4,  2,  6,  3],\n",
       "         [-1, -1, -1, -1]],\n",
       "\n",
       "        [[ 4,  3,  1,  3],\n",
       "         [ 6,  4,  9,  2],\n",
       "         [-1, -1, -1, -1]]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_batch = embedded_batch.masked_fill(src_mask == 0, -1)\n",
    "masked_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 0],\n",
       "        [4, 4, 0]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2, 1, 1, 4],\n",
       "         [7, 3, 3, 2],\n",
       "         [2, 2, 1, 4]],\n",
       "\n",
       "        [[6, 9, 4, 9],\n",
       "         [3, 4, 8, 6],\n",
       "         [5, 4, 4, 9]]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False],\n",
       "         [ True,  True, False],\n",
       "         [ True,  True,  True]]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "nopeak_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
